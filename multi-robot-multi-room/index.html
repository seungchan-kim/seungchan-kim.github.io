<!DOCTYPE html>
<html>
<head lang="en">
<meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Multi-Robot Multi-Room Exploration</title>

<meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://seungchan-kim.github.io/multi-robot-multi-room"/>
    <meta property="og:title" content="MRMR"/>
    <meta property="og:description" content="Project page for Multi-Robot Multi-Room Exploration with Structural Cue Extraction and Sphere Representations" />
        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="MRMR" />
    <meta name="twitter:description" content="Project page for Multi-Robot Multi-Room Exploration with Structural Cue Extraction and Sphere Representations" />
    <meta name="twitter:image" content="" />
<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
</head>
<body>
<div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>Multi-Robot Multi-Room Exploration with</b> <br><b>Geometric Cue Extraction and Circular Decomposition</b></br>
		<small>
                    IEEE Robotics and Automation Letters (RA-L)
                </small>
		</h2>
	</div> 
	<div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://seungchan-kim.github.io" target="_blank">
                          Seungchan Kim
                        </a><sup>1</sup>
                    </li>
                    <li>
                        <a href="" target="_blank">
                          Micah Corah
                        </a><sup>1</sup>
                    </li>
                    <li>
                        <a href="">
                          John Keller  
                        </a><sup>1</sup>
                    </li>
                    <li>
                        <a href="" target="_blank">
                          Graeme Best
                        </a><sup>2</sup>
                    </li>
                    <li>
                        <a href="" target="_blank">
                          Sebastian Scherer
                        </a><sup>1</sup>
                    </li>
			<br><sup>1</sup>Robotics Institute, Carnegie Mellon University 
			<br><sup>2</sup>University of Technology Sydney
                </ul>
            </div>
        </div>

	<div class="row">
            <div class="col-md-4 col-md-offset-4 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://arxiv.org/pdf/2307.15202.pdf" target="_blank">
                        <image src="figures/paper_1stpageview.png" height="60px">
                            <h4><strong>Paper<br></strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://youtu.be/zUtK1hh2Tpo?si=brgDVADz85odSC5V" target="_blank">
                        <image src="figures/youtube-logo.png" height="60px">
                            <h4><strong>Summary Video<br></strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>

	<div class="row">
            <div class="col-md-8 col-md-offset-2">
                <table align=center width="100%">
                    <tr>
                        <td colspan=7><video style="vertical-align: bottom;" autoplay="autoplay" controls="controls" loop="loop" muted="" playsinline="" src="figures/mrmr-overview.mp4" width="100%"></video></td>
                    </tr>
                </table>
                <br>
                <p class="text-justify">
		How can we make a robot autonomously explore an indoor environment, focusing on <b>"rooms"</b>? And how can we make <b>multiple robots</b> collectively explore as many rooms without redundancy under limited communication?
            </div>
        </div>
	
	<div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
			This work proposes an autonomous multi-robot exploration pipeline that coordinates the behaviors of robots in an indoor environment composed of multiple rooms. Contrary to simple frontier-based exploration approaches, we aim to enable robots to methodically explore and observe an unknown set of rooms in a structured building, keeping track of which rooms are already explored and sharing this information among robots to coordinate their behaviors in a distributed manner. To this end, we propose (1) a geometric cue extraction method that processes 3D point cloud data and detects the locations of potential cues such as doors and rooms, (2) a circular decomposition for free spaces used for target assignment. Using these two components, our pipeline effectively assigns tasks among robots, and enables a methodical exploration of rooms. We evaluate the performance of our pipeline using a team of up to 3 aerial robots, and show that our method outperforms the baseline by 33.4% in simulation and 26.4% in real-world experiments
		</p>
            </div>
        </div>

	<div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Perception and Data Processing
                </h3>
                <p class="text-justify">
			We focus on 3D LiDAR point cloud processing approach that enables high-speed, low-compute processing onboard.
                <p style="text-align:center;">
                    <image src="figures/perception-dataprocessing.png" class="img-responsive" alt="scales">
                </p>
                <p class="text-justify">
			An indoor environment is composed of multiple rooms. The exploring robot generates <b>3D occupancy voxel grid</b> from 3D point cloud data. The robot flattens 3D voxel grid map into 2D binary voxel grid map, and appiles median filtering. Then the robot generates <b>2D distance transform map</b>. We utilize this distance transform map to extract meaningful geometric features and cues for exploration.
                </p>
            </div>
        </div>

	<div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Door Detection using Saddle Point Extraction
                </h3>
		<table align=center width="100%">
                    <tr>
                        <td colspan=7><video style="vertical-align: bottom;" autoplay="autoplay" controls="controls" loop="loop" muted="" playsinline="" src="figures/drone-entering-door.mp4" width="100%"></video></td>
                    </tr>
                </table>
                <p class="text-justify">
			We model the doors as <em>saddle points</em> in the distance transform. 
		</p>
		<p>
			Think of a top-down view of a drone located at a door. And let's think about the distance to the closest occupied cells (which, in this case, are walls).
		</p>
                <table align=center width="100%">
                    <tr>
                        <td colspan=7><video style="vertical-align: bottom;" autoplay="autoplay" controls="controls" loop="loop" muted="" playsinline="" src="figures/door-saddle-points.mp4" width="100%"></video></td>
                    </tr>
                </table>
                <p class="text-justify">
			The point at the door has the farthest distance to the occupied cells, when compared to other points that lie on the axis of wall. This point has the smallest distance to the occupied cells, when compared to other points that lie on the perpendicular axis. In the distance transform space, the door(point) is at its maximum along one axis, and at its minimum along another perpendicular axis. Thus, we can model the doors as <b>saddle points</b> in the distance transform.
		</p>
            </div>
        </div>

	<div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Circular Decomposition of Free Space
                </h3>
		<p class="text-justify">
		    We represent the free space in rooms by decomposing it into circles. Extracting local maxima and distance to the occupied cells from distance transform map, robot generates circles that are tangent to the inner walls of the rooms via our circular decomposition method.  
		</p>
		<table align=center width="100%">
                    <tr>
                        <td colspan=7><video style="vertical-align: bottom;" autoplay="autoplay" controls="controls" loop="loop" muted="" playsinline="" src="figures/circles.mp4" width="100%"></video></td>
                    </tr>
                </table>
		<p class="text-justify">
		    The motivation behind this is that visiting the centers of the circles will enable the robot to observe most of the room. A circle, which is defined by a center and a radius, is also a compact representation of the space to maintain.  
		</p>
            </div>
        </div>

	<div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Multi-Robot Coordination
                </h3>
		<p class="text-justify">
			For multi-robot coordination, we make the robots share the doors (saddle points) and rooms (free space decomposed into circles) as necessary information. Each robot updates the doors it has reached and circles it has completed, and publishes this information to other robots; each robot also receives information on the doors and circles that were reached by other robots.
		</p>
		<table align=center width="100%">
                    <tr>
                        <td colspan=7><video style="vertical-align: bottom;" autoplay="autoplay" controls="controls" loop="loop" muted="" playsinline="" src="figures/multi-circles.mp4" width="100%"></video></td>
                    </tr>
                </table>
		<p class="text-justify">
			Communicating this information bidirectionally, each robot avoids targeting a door that was reached by itself and other robots. Likewise, each robot avoids targeting a circle that was visited by itself and other robots.  
		</p>
            </div>
        </div>

	<div class="row">
		<div class="col-md-8 col-md-offset-2">
			<h3>
				Qualitative Results
			</h3>
			<p style="text-align:center;">
				<image src="figures/baseline-ours-compare.png" class="img-responsive" alt="scales">
			</p>
			<p class="text-justify">
				Using the baseline (frontier-based exploration), robots first choose to travel corridors fast by moving straight because the goal is to simply increase the coverage. In contrary, using our methods, the robots quickly turn directions as the focus is to explore rooms. The robots first target doors and then enter each room to explore. 
			</p>
			<table align=center width="100%">
                    		<tr>
                        		<td colspan=7><video style="vertical-align: bottom;" autoplay="autoplay" controls="controls" loop="loop" muted="" playsinline="" src="figures/two-robots-sim.mp4" width="100%"></video></td>
                    		</tr>
                	</table>
			<p class="text-justify">
				Using our method, two robots simultaneously explore rooms in a building without redundancy. 
			</p>
			<table align=center width="100%">
                                <tr>
                                        <td colspan=7><video style="vertical-align: bottom;" autoplay="autoplay" controls="controls" loop="loop" muted="" playsinline="" src="figures/three-robots-sim.mp4" width="100%"></video></td>
                                </tr>
                        </table>
                        <p class="text-justify">
                                Using our method, three robots simultaneously explore rooms in a building without redundancy.
                        </p>
			<p style="text-align:center;">
				<image src="figures/robust-saddle.png" class="img-responsive" alt="scales">
			</p>
			<p class="text-justify">
				Our saddle point extraction algorithm is robust at detecting doors (a) not only in normal settings, (b) but also when the axes of walls are not aligned with x-axis and y-axis of global coordinate frame. This is because the detection of saddle points (which is obtained from second-order partial derivative test from 2D Hessian matrix) can be done irrespective of the wall's alignment with the global axes. It only requires that a function (in this case, distance to the occupied cells) has a local maximum in one direction, but a local minimum in another direction. These two directions should be perpendicular each other, but they don't necessarily have to be aligned with the global axes.  
			</p>
		</div>
	</div>
			
	<div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@misc{kim2023multirobot,
      title={Multi-Robot Multi-Room Exploration with Geometric Cue Extraction and Circular Decomposition}, 
      author={Seungchan Kim and Micah Corah and John Keller and Graeme Best and Sebastian Scherer},
      year={2023},
      eprint={2307.15202},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}</textarea>
                </div>
            </div>
        </div>

	<div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
		We thank Ian Higgins, Je Hon Tan, and Aditya Parandekar for helping with real-robot experiments. This work was supported by Defense Science and Technology Agency (DSTA) Singapore.
                <br>
                </p>
            </div>
        </div>
</div>
</body>

</html>
